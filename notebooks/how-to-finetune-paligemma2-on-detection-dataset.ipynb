{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LqvmtZPzyY1"
      },
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# Fine-tune PaliGemma2 on Object Detection Dataset\n",
        "\n",
        "---\n",
        "\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2412.03555-b31b1b.svg)](https://arxiv.org/abs/2412.03555)\n",
        "\n",
        "PaliGemma 2 is built by combining the SigLIP-So400m vision encoder with the more recent and capable language models from the Gemma 2 family.\n",
        "\n",
        "![PaliGemma2 Figure.1](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/paligemma2-1.png)\n",
        "\n",
        "The authors use a 3-stage training approach similar to the original PaliGemma. In stage 1, they combine the pretrained vision and language model components and train them jointly on a multimodal task mixture. In stage 2, they train the models at higher resolutions of 448px^2 and 896px^2. In stage 3, they fine-tune the models on the target transfer tasks.\n",
        "\n",
        "PaliGemma 2 models outperform the original PaliGemma at the same resolution and model size. Increasing the model size and resolution generally improves performance across a wide range of tasks, but the benefits differ depending on the task. Some tasks benefit more from increased resolution, while others benefit more from a larger language model.\n",
        "\n",
        "![PaliGemma2 Figure.2](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/paligemma2-2.png)\n",
        "\n",
        "Notebook requires A100 with 40GB of VRAM to train."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBp3Czz3GBmc"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADTkh-2y_9Yv"
      },
      "source": [
        "### Configure your API keys\n",
        "\n",
        "To fine-tune PaliGemma2, you need to provide your HuggingFace Token and Roboflow API key. Follow these steps:\n",
        "\n",
        "- Open your [`HuggingFace Settings`](https://huggingface.co/settings) page. Click `Access Tokens` then `New Token` to generate new token.\n",
        "- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy`. This will place your private key in the clipboard.\n",
        "- In Colab, go to the left pane and click on `Secrets` (🔑).\n",
        "    - Store HuggingFace Access Token under the name `HF_TOKEN`.\n",
        "    - Store Roboflow API Key under the name `ROBOFLOW_API_KEY`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wyojKiG_hX9"
      },
      "source": [
        "### Select the runtime\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `T4 GPU`, and then click `Save`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_8BLW6R_x-z",
        "outputId": "703f5dea-16dc-492a-cf37-9d3cd5909372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jan 25 02:33:31 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMlw3ru1YvLg"
      },
      "source": [
        "### Download dataset from Roboflow Universe\n",
        "\n",
        "To fine-tune PaliGemma2, prepare your dataset in JSONL format. You can use Roboflow to easily convert any dataset into this format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wtvz4QZ9YuG8",
        "outputId": "d2bdca82-ead7-42d1-9879-60e5bd62b8ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q roboflow supervision peft bitsandbytes transformers==4.47.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "TGDFTYVnY4zn",
        "outputId": "daeab6f6-149c-4b29-c7d7-0e786d8a9fe3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret QPtFXk2KR0s5bGzha4lM does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-946b210c844e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mroboflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRoboflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mROBOFLOW_API_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'QPtFXk2KR0s5bGzha4lM'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRoboflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mROBOFLOW_API_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret QPtFXk2KR0s5bGzha4lM does not exist."
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "from roboflow import Roboflow\n",
        "\n",
        "ROBOFLOW_API_KEY = userdata.get('rf_7r2XExNy4EMFp9KLEnUmHN1gRRh2')\n",
        "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
        "\n",
        "project = rf.workspace(\"roboflow-jvuqo\").project(\"poker-cards-fmjio\")\n",
        "version = project.version(4)\n",
        "dataset = version.download(\"paligemma\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Let's read the first few lines of the annotation file and examine the dataset format."
      ],
      "metadata": {
        "id": "Cm7ElnTKvBVa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLhSenP5AtQe"
      },
      "outputs": [],
      "source": [
        "!head -n 5 {dataset.location}/dataset/_annotations.train.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up and test data loaders"
      ],
      "metadata": {
        "id": "McU9159EvkeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class JSONLDataset(Dataset):\n",
        "    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n",
        "        self.jsonl_file_path = jsonl_file_path\n",
        "        self.image_directory_path = image_directory_path\n",
        "        self.entries = self._load_entries()\n",
        "\n",
        "    def _load_entries(self):\n",
        "        entries = []\n",
        "        with open(self.jsonl_file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                data = json.loads(line)\n",
        "                entries.append(data)\n",
        "        return entries\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        if idx < 0 or idx >= len(self.entries):\n",
        "            raise IndexError(\"Index out of range\")\n",
        "\n",
        "        entry = self.entries[idx]\n",
        "        image_path = os.path.join(self.image_directory_path, entry['image'])\n",
        "        image = Image.open(image_path)\n",
        "        return image, entry"
      ],
      "metadata": {
        "id": "pkDA3ZG5wBAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = JSONLDataset(\n",
        "    jsonl_file_path=f\"{dataset.location}/dataset/_annotations.train.jsonl\",\n",
        "    image_directory_path=f\"{dataset.location}/dataset\",\n",
        ")\n",
        "valid_dataset = JSONLDataset(\n",
        "    jsonl_file_path=f\"{dataset.location}/dataset/_annotations.valid.jsonl\",\n",
        "    image_directory_path=f\"{dataset.location}/dataset\",\n",
        ")\n",
        "test_dataset = JSONLDataset(\n",
        "    jsonl_file_path=f\"{dataset.location}/dataset/_annotations.test.jsonl\",\n",
        "    image_directory_path=f\"{dataset.location}/dataset\",\n",
        ")"
      ],
      "metadata": {
        "id": "m62CV8HRwsb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import supervision as sv\n",
        "\n",
        "\n",
        "CLASSES = train_dataset[0][1]['prefix'].replace(\"detect \", \"\").split(\" ; \")\n",
        "\n",
        "images = []\n",
        "for i in range(25):\n",
        "    image, label = train_dataset[i]\n",
        "    detections = sv.Detections.from_lmm(\n",
        "        lmm='paligemma',\n",
        "        result=label[\"suffix\"],\n",
        "        resolution_wh=(image.width, image.height),\n",
        "        classes=CLASSES)\n",
        "\n",
        "    image = sv.BoxAnnotator(thickness=4).annotate(image, detections)\n",
        "    image = sv.LabelAnnotator(text_scale=2, text_thickness=4).annotate(image, detections)\n",
        "    images.append(image)\n",
        "\n",
        "sv.plot_images_grid(images, (5, 5))"
      ],
      "metadata": {
        "id": "19pQQjIixL-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load PaliGemma2 model\n",
        "\n",
        "**NOTE:** PaliGemma2 offers 9 pre-trained models with sizes of `3B`, `10B`, and `28B` parameters, and resolutions of `224`, `448`, and `896` pixels. In this tutorial, I'll be using the [`google/paligemma2-3b-pt-448`](https://huggingface.co/google/paligemma2-3b-pt-448) checkpoint. Resolution has a key impact on the mAP of the trained model, and it seems that `448` offers the most optimal balance between performance and compute resources required to train the model."
      ],
      "metadata": {
        "id": "_ZvYNxYbBtE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
        "\n",
        "MODEL_ID =\"google/paligemma2-3b-pt-448\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "l0DpE_ibx9aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "VfCgxIp3EjmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = PaliGemmaProcessor.from_pretrained(MODEL_ID)"
      ],
      "metadata": {
        "id": "ntXj4A3SyEAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Depending on the multimodal task you are trying to solve, we have prepared two training versions that optimize VRAM consumption.\n",
        "\n",
        "- Freeze the image encoder and fine-tune only the text decoder.\n",
        "- Fine-tune the entire model with LoRA and QLoRA."
      ],
      "metadata": {
        "id": "DwLQf92pCDw6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRanM-7XMxSl"
      },
      "outputs": [],
      "source": [
        "# @title Freeze the image encoder\n",
        "\n",
        "\n",
        "TORCH_DTYPE = torch.bfloat16\n",
        "\n",
        "model = PaliGemmaForConditionalGeneration.from_pretrained(MODEL_ID, torch_dtype=TORCH_DTYPE).to(DEVICE)\n",
        "\n",
        "for param in model.vision_tower.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.multi_modal_projector.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Fine-tune the entire model with LoRA and QLoRA\n",
        "\n",
        "# from transformers import BitsAndBytesConfig\n",
        "# from peft import get_peft_model, LoraConfig\n",
        "\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
        "# )\n",
        "\n",
        "# lora_config = LoraConfig(\n",
        "#     r=8,\n",
        "#     target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "#     task_type=\"CAUSAL_LM\",\n",
        "# )\n",
        "\n",
        "# model = PaliGemmaForConditionalGeneration.from_pretrained(MODEL_ID, device_map=\"auto\")\n",
        "# model = get_peft_model(model, lora_config)\n",
        "# model.print_trainable_parameters()\n",
        "\n",
        "# TORCH_DTYPE = model.dtype"
      ],
      "metadata": {
        "id": "F3T6XCGvygcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tune PaliGemma2 on custom object detection dataset"
      ],
      "metadata": {
        "id": "DMgYuYeECkoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "def augment_suffix(suffix):\n",
        "    parts = suffix.split(' ; ')\n",
        "    random.shuffle(parts)\n",
        "    return ' ; '.join(parts)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, labels = zip(*batch)\n",
        "\n",
        "    paths = [label[\"image\"] for label in labels]\n",
        "    prefixes = [\"<image>\" + label[\"prefix\"] for label in labels]\n",
        "    suffixes = [augment_suffix(label[\"suffix\"]) for label in labels]\n",
        "\n",
        "    inputs = processor(\n",
        "        text=prefixes,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "        suffix=suffixes,\n",
        "        padding=\"longest\"\n",
        "    ).to(TORCH_DTYPE).to(DEVICE)\n",
        "\n",
        "    return inputs\n",
        "\n",
        "args = TrainingArguments(\n",
        "    num_train_epochs=16,\n",
        "    remove_unused_columns=False,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    warmup_steps=2,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=1e-6,\n",
        "    adam_beta2=0.999,\n",
        "    logging_steps=50,\n",
        "    optim=\"adamw_hf\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=1000,\n",
        "    save_total_limit=1,\n",
        "    output_dir=\"paligemma2_object_detection\",\n",
        "    bf16=True,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    dataloader_pin_memory=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    data_collator=collate_fn,\n",
        "    args=args\n",
        ")"
      ],
      "metadata": {
        "id": "AE6-8cHo2VYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "CMPT5vIWbSJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run inference with fine-tuned PaliGemma2 model"
      ],
      "metadata": {
        "id": "1c4FK0XXCxPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = test_dataset[1]\n",
        "prefix = \"<image>\" + label[\"prefix\"]\n",
        "suffix = label[\"suffix\"]\n",
        "\n",
        "inputs = processor(\n",
        "    text=prefix,\n",
        "    images=image,\n",
        "    return_tensors=\"pt\"\n",
        ").to(TORCH_DTYPE).to(DEVICE)\n",
        "\n",
        "prefix_length = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "with torch.inference_mode():\n",
        "    generation = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
        "    generation = generation[0][prefix_length:]\n",
        "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
        "    print(decoded)\n",
        "\n",
        "w, h = image.size\n",
        "detections = sv.Detections.from_lmm(\n",
        "    lmm='paligemma',\n",
        "    result=decoded,\n",
        "    resolution_wh=(w, h),\n",
        "    classes=CLASSES)\n",
        "\n",
        "annotated_image = image.copy()\n",
        "annotated_image = sv.BoxAnnotator().annotate(annotated_image, detections)\n",
        "annotated_image = sv.LabelAnnotator(smart_position=True).annotate(annotated_image, detections)\n",
        "annotated_image"
      ],
      "metadata": {
        "id": "ios5UoVg2hcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate fine-tuned PaliGemma2 model"
      ],
      "metadata": {
        "id": "5cU-C88_IfAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "images = []\n",
        "targets = []\n",
        "predictions = []\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for i in tqdm(range(len(test_dataset))):\n",
        "        image, label = test_dataset[i]\n",
        "        prefix = \"<image>\" + label[\"prefix\"]\n",
        "        suffix = label[\"suffix\"]\n",
        "\n",
        "        inputs = processor(\n",
        "            text=prefix,\n",
        "            images=image,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(TORCH_DTYPE).to(DEVICE)\n",
        "\n",
        "        prefix_length = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "        generation = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
        "        generation = generation[0][prefix_length:]\n",
        "        generated_text = processor.decode(generation, skip_special_tokens=True)\n",
        "\n",
        "        w, h = image.size\n",
        "        prediction = sv.Detections.from_lmm(\n",
        "            lmm='paligemma',\n",
        "            result=generated_text,\n",
        "            resolution_wh=(w, h),\n",
        "            classes=CLASSES)\n",
        "\n",
        "        prediction.class_id = np.array([CLASSES.index(class_name) for class_name in prediction['class_name']])\n",
        "        prediction.confidence = np.ones(len(prediction))\n",
        "\n",
        "        target = sv.Detections.from_lmm(\n",
        "            lmm='paligemma',\n",
        "            result=suffix,\n",
        "            resolution_wh=(w, h),\n",
        "            classes=CLASSES)\n",
        "\n",
        "        target.class_id = np.array([CLASSES.index(class_name) for class_name in target['class_name']])\n",
        "\n",
        "        images.append(image)\n",
        "        targets.append(target)\n",
        "        predictions.append(prediction)"
      ],
      "metadata": {
        "id": "6zUxJufnIVMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Calculate mAP\n",
        "\n",
        "from supervision.metrics import MeanAveragePrecision, MetricTarget\n",
        "\n",
        "map_metric = MeanAveragePrecision(metric_target=MetricTarget.BOXES)\n",
        "map_result = map_metric.update(predictions, targets).compute()\n",
        "\n",
        "print(map_result)"
      ],
      "metadata": {
        "id": "jRj1CYMBJ2GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_result.plot()"
      ],
      "metadata": {
        "id": "hgJglQicJ6lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Calculate Confusion Matrix\n",
        "confusion_matrix = sv.ConfusionMatrix.from_detections(\n",
        "    predictions=predictions,\n",
        "    targets=targets,\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "_ = confusion_matrix.plot()"
      ],
      "metadata": {
        "id": "HKW5wbAHKkp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_images = []\n",
        "\n",
        "for i in range(25):\n",
        "    image = images[i]\n",
        "    detections = predictions[i]\n",
        "\n",
        "    annotated_image = image.copy()\n",
        "    annotated_image = sv.BoxAnnotator(thickness=4).annotate(annotated_image, detections)\n",
        "    annotated_image = sv.LabelAnnotator(text_scale=2, text_thickness=4, smart_position=True).annotate(annotated_image, detections)\n",
        "    annotated_images.append(annotated_image)\n",
        "\n",
        "sv.plot_images_grid(annotated_images, (5, 5))"
      ],
      "metadata": {
        "id": "JiMGnAK1BUpY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}